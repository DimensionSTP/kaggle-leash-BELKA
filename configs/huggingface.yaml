# @package _global_
defaults:
  - dataset: huggingface_kaggleleashbelka_dataset
  - architecture: huggingface_architecture
  - tuner: huggingface_tuner
  - logger: wandb
  - hydra: hydra
  - callbacks: callbacks
  - trainer: trainer

package_name: kaggle-leash-BELKA
project_dir: ${oc.env:PROJECT_DIR}/${package_name}
connected_dir: ${oc.env:CONNECTED_DIR}/${package_name}

seed: 2024

num_labels: 3

split:
  train: train
  val: val
  test: test
  predict: predict

batch_size: 1024

num_samples: 2_000_000
split_ratio: 0.1
data_column_name: molecule_smiles
target_column_names:
  - BRD4
  - HSA
  - sEH
upload_user: DeepChem
model_type: ChemBERTa-10M-MTR
pretrained_model_name: ${upload_user}/${model_type}
data_max_length: 120

dropout_ratio: 0.1

lr: 0.0001
period: 2
eta_min: 0.00001

monitor: val_MultilabelF1Score
tracking_direction: max
patience: 2
min_delta: 0

devices: ${oc.decode:${oc.env:DEVICES}}
accelerator: gpu
strategy: ddp
log_every_n_steps: 10
precision: 32
accumulate_grad_batches: 1
gradient_clip_val: 1
gradient_clip_algorithm: norm
epoch: 100

model_name: HuggingFace
dataset_name: KaggleLeashBELKA
mode: train

is_tuned: tuned
num_trials: 3
hparams_save_path: ${connected_dir}/hparams/${model_name}/${dataset_name}/${num_trials}_trials
tuned_hparams_path: ${hparams_save_path}/best_params.json

project_name: ${model_name}-${dataset_name}-${mode}
total_batch_size: bs=${batch_size}x${devices}x${accumulate_grad_batches}
save_detail: ${upload_user}_${model_type}-precision=${precision}-${total_batch_size}
resumed_step: 0
ckpt_path: ${callbacks.model_checkpoint.dirpath}/epoch=${epoch}.ckpt

submission_file_name: test
label_type_column_name: protein_name
result_column_name: binds
order_column_name: id
num_rows: 180_000_000
submission_name: ${save_detail}-epoch=${epoch}

run_name: ${project_name}
work_dir: ${hydra:runtime.cwd}